{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52e07823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1 of 102\n",
      "Processed 11 of 102\n",
      "Processed 21 of 102\n",
      "Processed 31 of 102\n",
      "Processed 41 of 102\n",
      "Processed 51 of 102\n",
      "Processed 61 of 102\n",
      "Processed 71 of 102\n",
      "Processed 81 of 102\n",
      "Processed 91 of 102\n",
      "Processed 101 of 102\n",
      "✅ ENDOH_enriched.csv saved with Word Count + Frequency (5-year window).\n"
     ]
    }
   ],
   "source": [
    "# === ENRICH ENDOH CSV WITH WORD COUNT + PUBMED FREQUENCY (LAST 10 YEARS) ===\n",
    "\n",
    "import pandas as pd\n",
    "from Bio import Entrez\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Load original ENDOH.csv\n",
    "df = pd.read_csv(\"ENDOH.csv\")\n",
    "df = df[['Preferred Label', 'Parents']].dropna()\n",
    "df['Preferred Label'] = df['Preferred Label'].astype(str)\n",
    "\n",
    "# Compute Word Count (treat underscores as spaces)\n",
    "df['Word Count'] = df['Preferred Label'].apply(lambda x: len(x.replace('_', ' ').split()))\n",
    "\n",
    "# Set up Entrez API\n",
    "Entrez.api_key = \"ba9c6cd0806a467f30ca76b5ebd32531b508\"\n",
    "Entrez.email = \"name@example.com\"\n",
    "\n",
    "# Compute PubMed Frequency for the last 5 years\n",
    "start_year = datetime.now().year - 5\n",
    "end_year = datetime.now().year\n",
    "\n",
    "def get_pubmed_freq(term):\n",
    "    try:\n",
    "        query = f'\"{term.replace(\"_\", \" \")}\" AND ({start_year}[PDAT] : {end_year}[PDAT])'\n",
    "        handle = Entrez.esearch(db=\"pubmed\", term=query, retmax=1)\n",
    "        record = Entrez.read(handle)\n",
    "        handle.close()\n",
    "        return int(record[\"Count\"])\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "# Apply to all concepts\n",
    "frequencies = []\n",
    "for idx, term in enumerate(df['Preferred Label']):\n",
    "    freq = get_pubmed_freq(term)\n",
    "    frequencies.append(freq)\n",
    "    if idx % 10 == 0:\n",
    "        print(f\"Processed {idx + 1} of {len(df)}\")\n",
    "    time.sleep(0.34)  # stay within NCBI limit\n",
    "\n",
    "df['Frequency'] = frequencies\n",
    "\n",
    "# Save enriched file\n",
    "df.to_csv(\"ENDOH_enriched.csv\", index=False)\n",
    "print(\"✅ ENDOH_enriched.csv saved with Word Count + Frequency (5-year window).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d9944d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Stage 1 ===\n",
      "Concept: badly_maintained_urban_public_parks\n",
      "Most Similar Cluster: Accessibility_to_green_space\n",
      "Avg. Semantic Similarity: 0.5462\n",
      "Max Redundancy (Jaccard): 0.125 with 'loss_of_urban_forest'\n",
      "Utility Score (US): 0.4212\n"
     ]
    }
   ],
   "source": [
    "# === CELL 1: Stage 1 - Utility Score (US) ===\n",
    "\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load enriched CSV\n",
    "df = pd.read_csv(\"ENDOH_enriched.csv\")\n",
    "df = df[['Preferred Label', 'Parents', 'Word Count', 'Frequency']].dropna()\n",
    "df['Preferred Label'] = df['Preferred Label'].astype(str)\n",
    "df['Cluster'] = df['Parents'].apply(lambda x: x.strip().split('#')[-1])\n",
    "cluster_dict = df.groupby('Cluster')['Preferred Label'].apply(list).to_dict()\n",
    "\n",
    "# Concept and weights\n",
    "concept_x = \"badly_maintained_urban_public_parks\"\n",
    "w1 = 1.0  # Semantic similarity\n",
    "w2 = 1.0  # Redundancy\n",
    "\n",
    "# Embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "x_embedding = model.encode(concept_x.replace('_', ' '), convert_to_tensor=True)\n",
    "\n",
    "# Semantic similarity\n",
    "semantic_scores = {}\n",
    "for cluster_name, terms in cluster_dict.items():\n",
    "    cluster_embeddings = model.encode([t.replace('_', ' ') for t in terms], convert_to_tensor=True)\n",
    "    cosine_scores = util.cos_sim(x_embedding, cluster_embeddings)\n",
    "    avg_score = torch.mean(cosine_scores).item()\n",
    "    semantic_scores[cluster_name] = avg_score\n",
    "\n",
    "best_cluster = max(semantic_scores, key=semantic_scores.get)\n",
    "best_similarity = semantic_scores[best_cluster]\n",
    "\n",
    "# Redundancy (Jaccard) — FIXED to only use the best cluster\n",
    "def jaccard_sim(a, b):\n",
    "    a_words = set(a.lower().split('_'))\n",
    "    b_words = set(b.lower().split('_'))\n",
    "    union = a_words | b_words\n",
    "    intersection = a_words & b_words\n",
    "    return len(intersection) / len(union) if union else 0\n",
    "\n",
    "max_redundancy = -1\n",
    "most_redundant_concept = \"\"\n",
    "for term in cluster_dict[best_cluster]:\n",
    "    score = jaccard_sim(concept_x, term)\n",
    "    if score > max_redundancy and term != concept_x:\n",
    "        max_redundancy = score\n",
    "        most_redundant_concept = term\n",
    "\n",
    "# Utility Score\n",
    "utility_score = (w1 * best_similarity) - (w2 * max_redundancy)\n",
    "\n",
    "# Word count & frequency stats from seed ontology\n",
    "word_stats = df['Word Count'].describe()\n",
    "freq_stats = df['Frequency'].describe()\n",
    "\n",
    "seed_stats = {\n",
    "    'mean_wc': word_stats['mean'], 'std_wc': word_stats['std'],\n",
    "    'min_wc': word_stats['min'], 'max_wc': word_stats['max'],\n",
    "    'mean_freq': freq_stats['mean'], 'std_freq': freq_stats['std'],\n",
    "    'min_freq': freq_stats['min'], 'max_freq': freq_stats['max']\n",
    "}\n",
    "\n",
    "print(\"\\n=== Stage 1 ===\")\n",
    "print(\"Concept:\", concept_x)\n",
    "print(\"Most Similar Cluster:\", best_cluster)\n",
    "print(\"Avg. Semantic Similarity:\", round(best_similarity, 4))\n",
    "print(\"Max Redundancy (Jaccard):\", round(max_redundancy, 4), f\"with '{most_redundant_concept}'\")\n",
    "print(\"Utility Score (US):\", round(utility_score, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f8261ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Stage 2 ===\n",
      "German Translation: schlecht gepflegte städtische öffentliche Parks\n",
      "Raw Word Count: 5 (Seed min: 1.0, max: 6.0)\n",
      "Raw PubMed Frequency: 0 (Seed min: 0.0, max: 532625.0)\n",
      "\n",
      "Combination Score: 0.4\n",
      "Normalized Word Count: 0.2\n",
      "Translation Quality Score: 1.0\n",
      "Normalized Frequency: 0\n",
      "\n",
      "🎯 Final Goodness Score: 0.414 [Weights: α=0.15, β=0.22, λ=0.31, θ=0.27]\n"
     ]
    }
   ],
   "source": [
    "# === CELL 2: Stage 2 - Goodness Score (Improved) ===\n",
    "\n",
    "import requests\n",
    "import re\n",
    "from Bio import Entrez\n",
    "\n",
    "# === API Setup ===\n",
    "merriam_key = \"18cad792-e991-4203-a8a7-b41746f1d538\"\n",
    "entrez_key = \"ba9c6cd0806a467f30ca76b5ebd32531b508\"\n",
    "Entrez.api_key = entrez_key\n",
    "Entrez.email = \"nk88@njit.edu\"\n",
    "\n",
    "# === Valid POS Combinations ===\n",
    "valid_combos = {\n",
    "    frozenset(['noun', 'noun']): 1.0,\n",
    "    frozenset(['adjective', 'noun']): 0.95,\n",
    "    frozenset(['noun', 'noun', 'noun']): 0.9,\n",
    "    frozenset(['verb', 'noun']): 0.85,\n",
    "    frozenset(['noun', 'adjective']): 0.8,\n",
    "    frozenset(['noun', 'verb']): 0.75,\n",
    "    frozenset(['adjective', 'noun', 'noun']): 0.7,\n",
    "    frozenset(['adjective', 'adjective', 'noun']): 0.65,\n",
    "    frozenset(['noun', 'prepositional phrase']): 0.6,\n",
    "    frozenset(['adjective', 'adjective', 'adjective', 'noun']): 0.55,\n",
    "    frozenset(['noun', 'noun', 'prepositional phrase']): 0.5,\n",
    "    frozenset(['adjective', 'noun', 'noun', 'noun']): 0.45,\n",
    "    frozenset(['noun', 'noun', 'noun', 'noun']): 0.4,\n",
    "    frozenset(['noun', 'adjective', 'noun', 'noun']): 0.35,\n",
    "    frozenset(['adjective', 'noun', 'noun', 'noun', 'noun']): 0.3\n",
    "}\n",
    "\n",
    "# === Merriam-Webster Combination Score ===\n",
    "def check_merriam(term):\n",
    "    url = f\"https://www.dictionaryapi.com/api/v3/references/medical/json/{term}?key={merriam_key}\"\n",
    "    try:\n",
    "        r = requests.get(url).json()\n",
    "        for entry in r:\n",
    "            if isinstance(entry, dict) and 'meta' in entry and entry['meta']['id'] == term:\n",
    "                if 'fl' in entry:\n",
    "                    return entry['fl']\n",
    "        return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def combination_score(term):\n",
    "    tags = []\n",
    "    for word in term.replace('_', ' ').split():\n",
    "        tag = check_merriam(word)\n",
    "        if tag:\n",
    "            tags.append(tag)\n",
    "    pos_set = frozenset(tags)\n",
    "    for combo in valid_combos:\n",
    "        if pos_set.issubset(combo):\n",
    "            return valid_combos[combo]\n",
    "    return 0.0\n",
    "\n",
    "# === Normalize Functions ===\n",
    "def normalize_freq(f, stats):\n",
    "    return max(0, min(1, (f - stats['min_freq']) / (stats['max_freq'] - stats['min_freq'])))\n",
    "\n",
    "def normalize_wc(wc, stats):\n",
    "    return max(0, min(1, 1 - ((wc - stats['min_wc']) / (stats['max_wc'] - stats['min_wc']))))\n",
    "\n",
    "# === Google Translate API ===\n",
    "def translate_to_german(term):\n",
    "    try:\n",
    "        url = \"https://translate.googleapis.com/translate_a/single\"\n",
    "        params = {\"client\": \"gtx\", \"sl\": \"en\", \"tl\": \"de\", \"dt\": \"t\", \"q\": term.replace('_', ' ')}\n",
    "        res = requests.get(url, params=params).json()\n",
    "        return res[0][0][0]\n",
    "    except:\n",
    "        return term\n",
    "\n",
    "# === Word Utilities ===\n",
    "def word_count(term):\n",
    "    if not isinstance(term, str):\n",
    "        return 0\n",
    "    cleaned_term = re.sub(r'[^A-Za-z0-9\\\\s]', '', term)\n",
    "    return len(cleaned_term.split())\n",
    "\n",
    "def extract_words(term):\n",
    "    if not isinstance(term, str):\n",
    "        return set()\n",
    "    cleaned_term = re.sub(r'[^A-Za-z0-9\\\\s]', '', term)\n",
    "    return set(cleaned_term.split())\n",
    "\n",
    "def decompose_german_term(term):\n",
    "    if not isinstance(term, str):\n",
    "        return set()\n",
    "    segments = re.findall(r'[A-ZÄÖÜa-zäöüß]+', term)\n",
    "    return set(segments)\n",
    "\n",
    "def is_compound_word(eng_term, ger_term):\n",
    "    eng_words = extract_words(eng_term)\n",
    "    ger_words = decompose_german_term(ger_term)\n",
    "    return bool(eng_words & ger_words)\n",
    "\n",
    "# === Translation Quality Score ===\n",
    "def translation_score(eng_term, ger_term):\n",
    "    eng_wc = word_count(eng_term)\n",
    "    ger_wc = word_count(ger_term)\n",
    "    score = 0.0\n",
    "\n",
    "    # Case 1: Short concepts (1-3 words)\n",
    "    if 1 <= eng_wc <= 3:\n",
    "        if ger_wc <= eng_wc:\n",
    "            score = 1.0\n",
    "        elif ger_wc <= eng_wc + 1:\n",
    "            score = 0.8\n",
    "        else:\n",
    "            score = 0.5\n",
    "\n",
    "    # Case 2: Medium concepts (4-6 words)\n",
    "    elif 4 <= eng_wc <= 6:\n",
    "        if ger_wc < eng_wc:\n",
    "            score = 1.0\n",
    "        elif ger_wc == eng_wc:\n",
    "            score = 0.8\n",
    "        else:\n",
    "            score = 0.5\n",
    "\n",
    "    # Case 3: Longer concepts (7-20 words)\n",
    "    elif 7 <= eng_wc <= 20:\n",
    "        if ger_wc < eng_wc * 0.8:\n",
    "            score = 0.9\n",
    "        elif ger_wc < eng_wc:\n",
    "            score = 0.7\n",
    "        else:\n",
    "            score = 0.4\n",
    "\n",
    "    # Case 4: Very long concepts (21-80 words)\n",
    "    elif 21 <= eng_wc <= 80:\n",
    "        if ger_wc < eng_wc * 0.8:\n",
    "            score = 0.7\n",
    "        else:\n",
    "            score = 0.4\n",
    "\n",
    "    # Bonus: Compound word check\n",
    "    if is_compound_word(eng_term, ger_term):\n",
    "        score += 0.1\n",
    "\n",
    "    return min(score, 1.0)\n",
    "\n",
    "# === Final Goodness Score Calculation ===\n",
    "alpha, beta, lambd, theta = 0.15, 0.22, 0.31, 0.27\n",
    "\n",
    "combo = combination_score(concept_x)\n",
    "freq = get_pubmed_freq(concept_x)\n",
    "norm_freq = normalize_freq(freq, seed_stats)\n",
    "wc = len(concept_x.replace('_', ' ').split())\n",
    "norm_wc = normalize_wc(wc, seed_stats)\n",
    "german = translate_to_german(concept_x)\n",
    "tscore = translation_score(concept_x, german)\n",
    "\n",
    "goodness = (alpha * combo) + (beta * norm_wc) + (lambd * tscore) + (theta * norm_freq)\n",
    "\n",
    "print(\"\\n=== Stage 2 ===\")\n",
    "print(f\"German Translation: {german}\")\n",
    "print(f\"Raw Word Count: {wc} (Seed min: {seed_stats['min_wc']}, max: {seed_stats['max_wc']})\")\n",
    "print(f\"Raw PubMed Frequency: {freq} (Seed min: {seed_stats['min_freq']}, max: {seed_stats['max_freq']})\\n\")\n",
    "\n",
    "print(f\"Combination Score: {round(combo, 3)}\")\n",
    "print(f\"Normalized Word Count: {round(norm_wc, 3)}\")\n",
    "print(f\"Translation Quality Score: {round(tscore, 3)}\")\n",
    "print(f\"Normalized Frequency: {round(norm_freq, 3)}\")\n",
    "\n",
    "print(f\"\\n🎯 Final Goodness Score: {round(goodness, 4)} [Weights: α={alpha}, β={beta}, λ={lambd}, θ={theta}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "628f154c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thresholds (you can tweak based on experiments)\n",
    "STAGE1_THRESHOLD = 0.17  # Utility Score threshold\n",
    "STAGE2_THRESHOLD = 0.45   # Goodness Score threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0712a820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Concept Classification ===\n",
      "⚠️ Passes Stage 1 only – Semantically relevant but lacks other goodness criteria.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Concept Classification ===\")\n",
    "if utility_score >= STAGE1_THRESHOLD and goodness >= STAGE2_THRESHOLD:\n",
    "    print(\"✅ Passes Stage 1 and Stage 2 – Strong candidate for inclusion.\")\n",
    "elif utility_score >= STAGE1_THRESHOLD and goodness < STAGE2_THRESHOLD:\n",
    "    print(\"⚠️ Passes Stage 1 only – Semantically relevant but lacks other goodness criteria.\")\n",
    "else:\n",
    "    print(\"❌ Fails Stage 1 – Not a useful or unique enough concept.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2920e28c",
   "metadata": {},
   "source": [
    "GOOD- climate_change, noise_regulations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b71700",
   "metadata": {},
   "source": [
    "BAD - housing_pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146bd391",
   "metadata": {},
   "outputs": [],
   "source": [
    "only stage 1 - badly_maintained_urban_public_parks\n",
    "Extremely verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e833549",
   "metadata": {},
   "source": [
    "Let’s break this down into our 3 categories:\n",
    "\n",
    "⸻\n",
    "\n",
    "**Categories:**\n",
    "\n",
    "- Pass Stage 1 and Stage 2 → Accept as good concept.\n",
    "- Pass Stage 1 only → Useful but may need refinement or more validation.\n",
    "- Fail Stage 1 → Likely irrelevant or redundant concept.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4343e35e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Concept Env",
   "language": "python",
   "name": "concept_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
