{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8beaa60e",
   "metadata": {},
   "source": [
    "# Concept Utility Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363d6da2",
   "metadata": {},
   "source": [
    "### ENRICH ENDOH CSV WITH WORD COUNT + PUBMED FREQUENCY (LAST 5 YEARS) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fbc21e",
   "metadata": {},
   "source": [
    "**Importing necessary dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "698a79a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Bio import Entrez\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dd7c9e",
   "metadata": {},
   "source": [
    "**Load original ENDOH.csv**\n",
    "\n",
    "EnDOH.csv consists of the current version of the Environmental Determinants of Health (EnDOH) from BioPortal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c4c75689",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"ENDOH.csv\")\n",
    "df = df[['Preferred Label', 'Parents']].dropna()\n",
    "df['Preferred Label'] = df['Preferred Label'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b20186",
   "metadata": {},
   "source": [
    "**Compute Word Count (treat underscores as spaces)**\n",
    "\n",
    "Here we compute the word count for each concept present in EnDOH "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3010998a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Word Count'] = df['Preferred Label'].apply(lambda x: len(x.replace('_', ' ').split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104c8a8c",
   "metadata": {},
   "source": [
    "**Set up Entrez API**\n",
    "\n",
    "API keys to retrieve PubMed frequency of occurence for all the concepts present in the seed ontology (EnDOH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bf3148bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Entrez.api_key = \"ba9c6cd0806a467f30ca76b5ebd32531b508\"\n",
    "Entrez.email = \"name@example.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e69a37",
   "metadata": {},
   "source": [
    "**Compute PubMed Frequency for the last 5 years**\n",
    "\n",
    "Here we are targeting the retrieval of PubMed Frequency of occurence of a concept for the last 5 year as its necessary to consider the recent developemnts/presence of concepts in the research domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7dbfed9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1 of 102\n",
      "Processed 11 of 102\n",
      "Processed 21 of 102\n",
      "Processed 31 of 102\n",
      "Processed 41 of 102\n",
      "Processed 51 of 102\n",
      "Processed 61 of 102\n",
      "Processed 71 of 102\n",
      "Processed 81 of 102\n",
      "Processed 91 of 102\n",
      "Processed 101 of 102\n"
     ]
    }
   ],
   "source": [
    "start_year = datetime.now().year - 5\n",
    "end_year = datetime.now().year\n",
    "\n",
    "def get_pubmed_freq(term):\n",
    "    try:\n",
    "        query = f'\"{term.replace(\"_\", \" \")}\" AND ({start_year}[PDAT] : {end_year}[PDAT])'\n",
    "        handle = Entrez.esearch(db=\"pubmed\", term=query, retmax=1)\n",
    "        record = Entrez.read(handle)\n",
    "        handle.close()\n",
    "        return int(record[\"Count\"])\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "# Apply to all concepts\n",
    "frequencies = []\n",
    "for idx, term in enumerate(df['Preferred Label']):\n",
    "    freq = get_pubmed_freq(term)\n",
    "    frequencies.append(freq)\n",
    "    if idx % 10 == 0:\n",
    "        print(f\"Processed {idx + 1} of {len(df)}\")\n",
    "    time.sleep(0.34)  # stay within NCBI limit\n",
    "\n",
    "df['Frequency'] = frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2613b9",
   "metadata": {},
   "source": [
    "**Save enriched file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ff8f24a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ENDOH_enriched.csv saved with Word Count + Frequency (5-year window).\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(\"ENDOH_enriched.csv\", index=False)\n",
    "print(\"✅ ENDOH_enriched.csv saved with Word Count + Frequency (5-year window).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f03a04f",
   "metadata": {},
   "source": [
    "### CELL 1: Stage 1 - Utility Score (US)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e22c8ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07adcae3",
   "metadata": {},
   "source": [
    "**Load enriched CSV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fe103ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"ENDOH_enriched.csv\")\n",
    "df = df[['Preferred Label', 'Parents', 'Word Count', 'Frequency']].dropna()\n",
    "df['Preferred Label'] = df['Preferred Label'].astype(str)\n",
    "df['Cluster'] = df['Parents'].apply(lambda x: x.strip().split('#')[-1])\n",
    "cluster_dict = df.groupby('Cluster')['Preferred Label'].apply(list).to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab950f9",
   "metadata": {},
   "source": [
    "**Concept and weights**\n",
    "\n",
    "Here we add weights to the Utility Score formula's two parameters which are \n",
    "\n",
    "\t•\tSemantic Similarity to a cluster (how well this concept fits into an existing topic group) - w1\n",
    "\t•\tRedundancy (how much this concept overlaps in wording with existing concepts, which is bad) - w2\n",
    "    \n",
    "**concept_x** represents the concept we want to evaluate for inclusion in the ontology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "73901a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_x = \"badly_maintained_urban_public_parks\"\n",
    "w1 = 1.0  # Semantic similarity\n",
    "w2 = 1.0  # Redundancy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cad3550",
   "metadata": {},
   "source": [
    "**Embedding model**\n",
    "\n",
    "Here we Load a pre-trained sentence embedding model from the Sentence Transformers library. 'all-MiniLM-L6-v2' is a lightweight and fast model with good performance for semantic similarity tasks.\n",
    "\n",
    "- Convert the concept from underscore_case to normal spaced text for better language model interpretation.\n",
    "- Then encode it into a vector (embedding) that represents the meaning of the phrase.\n",
    "- This embedding will later be compared to other concept embeddings to measure similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a5b09cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "x_embedding = model.encode(concept_x.replace('_', ' '), convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b824b88",
   "metadata": {},
   "source": [
    "**Semantic similarity**\n",
    "\n",
    "For each sub hierarchy, it computes the average cosine similarity between the embedding of concept_x and the embeddings of all concepts already present in that cluster. Before encoding, underscores in concept labels are replaced with spaces to better match the language model’s training data. \n",
    "\n",
    "The Sentence-BERT model (all-MiniLM-L6-v2) is used to generate these embeddings. The resulting average similarity score represents how close in meaning the test concept is to that cluster. \n",
    "\n",
    "The sub hierarchy with the highest average similarity is selected as the “best fit” for concept_x, and this highest similarity value becomes the best_similarity score. This value is a key component of the Stage 1 utility score, as it reflects the semantic alignment of the new concept with existing structures in the ontology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6d1f28b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_scores = {}\n",
    "for cluster_name, terms in cluster_dict.items():\n",
    "    cluster_embeddings = model.encode([t.replace('_', ' ') for t in terms], convert_to_tensor=True)\n",
    "    cosine_scores = util.cos_sim(x_embedding, cluster_embeddings)\n",
    "    avg_score = torch.mean(cosine_scores).item()\n",
    "    semantic_scores[cluster_name] = avg_score\n",
    "\n",
    "best_cluster = max(semantic_scores, key=semantic_scores.get)\n",
    "best_similarity = semantic_scores[best_cluster]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d25e873",
   "metadata": {},
   "source": [
    "**Redundancy (Jaccard) — only use the best (semantically similar) sub hierarchy**\n",
    "\n",
    "This section calculates the redundancy of the concept based on Jaccard similarity, but only within the cluster identified as the most semantically similar (i.e., the best sub-hierarchy for concept_x). Redundancy is measured by comparing the words in concept_x to the words in each concept already present in that cluster. For each comparison, the Jaccard similarity is computed as the size of the intersection divided by the size of the union of the word sets. The highest redundancy score across all comparisons is retained as max_redundancy. \n",
    "\n",
    "By focusing only on the most semantically relevant cluster, the algorithm avoids penalizing the concept for similarities with unrelated parts of the ontology, and ensures that redundancy is assessed in the most contextually appropriate way. This score is subtracted from the semantic similarity to calculate the final utility score in Stage 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "12036929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_sim(a, b):\n",
    "    a_words = set(a.lower().split('_'))\n",
    "    b_words = set(b.lower().split('_'))\n",
    "    union = a_words | b_words\n",
    "    intersection = a_words & b_words\n",
    "    return len(intersection) / len(union) if union else 0\n",
    "\n",
    "max_redundancy = -1\n",
    "most_redundant_concept = \"\"\n",
    "for term in cluster_dict[best_cluster]:\n",
    "    score = jaccard_sim(concept_x, term)\n",
    "    if score > max_redundancy and term != concept_x:\n",
    "        max_redundancy = score\n",
    "        most_redundant_concept = term"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1071b8c",
   "metadata": {},
   "source": [
    "**Utility Score**\n",
    "\n",
    "Calculate the **utility_score** based on the **semantic similarity score** and the **maximum redundancy score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8aa064e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "utility_score = (w1 * best_similarity) - (w2 * max_redundancy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8e083d",
   "metadata": {},
   "source": [
    "**Word count & frequency stats from seed ontology**\n",
    "\n",
    "This code calculates descriptive statistics for the two key quantitative features used in Stage 2 of the Goodness Score: word count and PubMed frequency. By using the .describe() function on the entire seed ontology, it extracts values such as the mean, standard deviation, minimum, and maximum for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "33af13c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_stats = df['Word Count'].describe()\n",
    "freq_stats = df['Frequency'].describe()\n",
    "\n",
    "seed_stats = {\n",
    "    'mean_wc': word_stats['mean'], 'std_wc': word_stats['std'],\n",
    "    'min_wc': word_stats['min'], 'max_wc': word_stats['max'],\n",
    "    'mean_freq': freq_stats['mean'], 'std_freq': freq_stats['std'],\n",
    "    'min_freq': freq_stats['min'], 'max_freq': freq_stats['max']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937f9ea3",
   "metadata": {},
   "source": [
    "## Stage 1 - Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7b624b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Stage 1 ===\n",
      "Concept: badly_maintained_urban_public_parks\n",
      "Most Similar Cluster: Accessibility_to_green_space\n",
      "Avg. Semantic Similarity: 0.5462\n",
      "Max Redundancy (Jaccard): 0.125 with 'loss_of_urban_forest'\n",
      "Utility Score (US): 0.4212\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Stage 1 ===\")\n",
    "print(\"Concept:\", concept_x)\n",
    "print(\"Most Similar Cluster:\", best_cluster)\n",
    "print(\"Avg. Semantic Similarity:\", round(best_similarity, 4))\n",
    "print(\"Max Redundancy (Jaccard):\", round(max_redundancy, 4), f\"with '{most_redundant_concept}'\")\n",
    "print(\"Utility Score (US):\", round(utility_score, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe34c3d",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56307be",
   "metadata": {},
   "source": [
    "### CELL 2: Stage 2 - Goodness Score (Improved) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "40a63ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from Bio import Entrez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0270ab9",
   "metadata": {},
   "source": [
    "**API keys Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d412cf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "merriam_key = \"18cad792-e991-4203-a8a7-b41746f1d538\"\n",
    "entrez_key = \"ba9c6cd0806a467f30ca76b5ebd32531b508\"\n",
    "Entrez.api_key = entrez_key\n",
    "Entrez.email = \"nk88@njit.edu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfb78ee",
   "metadata": {},
   "source": [
    "**Valid POS Combinations**\n",
    "\n",
    "A set of predefined part-of-speech (POS) patterns that are commonly observed in well-structured multi-word concepts, particularly in biomedical and ontological contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "96ade3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_combos = {\n",
    "    frozenset(['noun', 'noun']): 1.0,\n",
    "    frozenset(['adjective', 'noun']): 0.95,\n",
    "    frozenset(['noun', 'noun', 'noun']): 0.9,\n",
    "    frozenset(['verb', 'noun']): 0.85,\n",
    "    frozenset(['noun', 'adjective']): 0.8,\n",
    "    frozenset(['noun', 'verb']): 0.75,\n",
    "    frozenset(['adjective', 'noun', 'noun']): 0.7,\n",
    "    frozenset(['adjective', 'adjective', 'noun']): 0.65,\n",
    "    frozenset(['noun', 'prepositional phrase']): 0.6,\n",
    "    frozenset(['adjective', 'adjective', 'adjective', 'noun']): 0.55,\n",
    "    frozenset(['noun', 'noun', 'prepositional phrase']): 0.5,\n",
    "    frozenset(['adjective', 'noun', 'noun', 'noun']): 0.45,\n",
    "    frozenset(['noun', 'noun', 'noun', 'noun']): 0.4,\n",
    "    frozenset(['noun', 'adjective', 'noun', 'noun']): 0.35,\n",
    "    frozenset(['adjective', 'noun', 'noun', 'noun', 'noun']): 0.3\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494bd8ca",
   "metadata": {},
   "source": [
    "**Merriam-Webster Combination Score**\n",
    "\n",
    "This section defines the logic used to calculate the Combination Score for a given concept, based on the part-of-speech (POS) tags of its component words. The check_merriam() function queries the Merriam-Webster Medical Dictionary API to retrieve the POS tag ('fl' = functional label) of a given term. The combination_score() function then splits a multi-word concept (e.g., \"air_pollution\") into its individual words, looks up each word’s POS using the API, and stores the results. \n",
    "\n",
    "It then forms a set of the POS tags and checks whether that set is a subset of any valid pattern defined in the valid_combos dictionary. If a match is found, it returns the corresponding score (e.g., 1.0 for noun-noun); if no match is found, the function returns a score of 0.0. This scoring mechanism rewards concepts that are linguistically clean and commonly interpretable, reinforcing the idea that syntactic structure is an important dimension of concept “goodness.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "adea0355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_merriam(term):\n",
    "    url = f\"https://www.dictionaryapi.com/api/v3/references/medical/json/{term}?key={merriam_key}\"\n",
    "    try:\n",
    "        r = requests.get(url).json()\n",
    "        for entry in r:\n",
    "            if isinstance(entry, dict) and 'meta' in entry and entry['meta']['id'] == term:\n",
    "                if 'fl' in entry:\n",
    "                    return entry['fl']\n",
    "        return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def combination_score(term):\n",
    "    tags = []\n",
    "    for word in term.replace('_', ' ').split():\n",
    "        tag = check_merriam(word)\n",
    "        if tag:\n",
    "            tags.append(tag)\n",
    "    pos_set = frozenset(tags)\n",
    "    for combo in valid_combos:\n",
    "        if pos_set.issubset(combo):\n",
    "            return valid_combos[combo]\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def83e44",
   "metadata": {},
   "source": [
    "**Normalize Functions**\n",
    "\n",
    "Normalize the frequency of occurence and the word count based on the maximum frequency and number of words from the seed ontology (in this case EnDOH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bd672bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_freq(f, stats):\n",
    "    return max(0, min(1, (f - stats['min_freq']) / (stats['max_freq'] - stats['min_freq'])))\n",
    "\n",
    "def normalize_wc(wc, stats):\n",
    "    return max(0, min(1, 1 - ((wc - stats['min_wc']) / (stats['max_wc'] - stats['min_wc']))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5852057c",
   "metadata": {},
   "source": [
    "**GPT 4.5 API**\n",
    "\n",
    "Retrieve the translation of the concept from the ontology in German from the GPT 4.5 API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7b9dce71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Set your OpenAI API key\n",
    "client = openai.OpenAI(api_key=\"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\")  # Replace with your actual key\n",
    "\n",
    "def translate_to_german(term):\n",
    "    prompt = f\"Translate the following English concept into fluent German. Only return the translation. No explanation or punctuation.\\n\\n{term.replace('_', ' ')}\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4-1106-preview\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.3\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"[Error translating '{term}']: {e}\")\n",
    "        return term"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e21336d",
   "metadata": {},
   "source": [
    "**Word Utilities**\n",
    "\n",
    "These utility functions support the evaluation of translation quality between English and German terms, which is a key factor in the Stage 2 Goodness Score. The word_count() function returns the number of words in a cleaned version of the term, filtering out non-alphanumeric characters. extract_words() performs similar cleaning but returns a set of the distinct words for use in comparisons. decompose_german_term() attempts to segment German compound words into individual components using regex, capturing both uppercase and lowercase alphabetic characters, including special German characters like umlauts. \n",
    "\n",
    "Finally, is_compound_word() compares the word components of an English term with the decomposed segments of its German translation and returns True if they share any common elements. \n",
    "\n",
    "This function helps determine whether a German term meaningfully reflects the structure or key elements of its English counterpart—offering a bonus signal when the translation preserves conceptual clarity through compound structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3f0d8dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(term):\n",
    "    if not isinstance(term, str):\n",
    "        return 0\n",
    "    cleaned_term = re.sub(r'[^A-Za-z0-9\\\\s]', '', term)\n",
    "    return len(cleaned_term.split())\n",
    "\n",
    "def extract_words(term):\n",
    "    if not isinstance(term, str):\n",
    "        return set()\n",
    "    cleaned_term = re.sub(r'[^A-Za-z0-9\\\\s]', '', term)\n",
    "    return set(cleaned_term.split())\n",
    "\n",
    "def decompose_german_term(term):\n",
    "    if not isinstance(term, str):\n",
    "        return set()\n",
    "    segments = re.findall(r'[A-ZÄÖÜa-zäöüß]+', term)\n",
    "    return set(segments)\n",
    "\n",
    "def is_compound_word(eng_term, ger_term):\n",
    "    eng_words = extract_words(eng_term)\n",
    "    ger_words = decompose_german_term(ger_term)\n",
    "    return bool(eng_words & ger_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1600743",
   "metadata": {},
   "source": [
    "**Translation Quality Score**\n",
    "\n",
    "Assign a translation quality score based on the cases mentioned below. The cases are based on the comparsion of the concept's number of words before (in English) and after the translation (in German)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9dba71cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translation_score(eng_term, ger_term):\n",
    "    eng_wc = word_count(eng_term)\n",
    "    ger_wc = word_count(ger_term)\n",
    "    score = 0.0\n",
    "\n",
    "    # Case 1: Short concepts (1-3 words)\n",
    "    if 1 <= eng_wc <= 3:\n",
    "        if ger_wc <= eng_wc:\n",
    "            score = 1.0\n",
    "        elif ger_wc <= eng_wc + 1:\n",
    "            score = 0.8\n",
    "        else:\n",
    "            score = 0.5\n",
    "\n",
    "    # Case 2: Medium concepts (4-6 words)\n",
    "    elif 4 <= eng_wc <= 6:\n",
    "        if ger_wc < eng_wc:\n",
    "            score = 1.0\n",
    "        elif ger_wc == eng_wc:\n",
    "            score = 0.8\n",
    "        else:\n",
    "            score = 0.5\n",
    "\n",
    "    # Case 3: Longer concepts (7-20 words)\n",
    "    elif 7 <= eng_wc <= 20:\n",
    "        if ger_wc < eng_wc * 0.8:\n",
    "            score = 0.9\n",
    "        elif ger_wc < eng_wc:\n",
    "            score = 0.7\n",
    "        else:\n",
    "            score = 0.4\n",
    "\n",
    "    # Case 4: Very long concepts (21-80 words)\n",
    "    elif 21 <= eng_wc <= 80:\n",
    "        if ger_wc < eng_wc * 0.8:\n",
    "            score = 0.7\n",
    "        else:\n",
    "            score = 0.4\n",
    "\n",
    "    # Bonus: Compound word check\n",
    "    if is_compound_word(eng_term, ger_term):\n",
    "        score += 0.1\n",
    "\n",
    "    return min(score, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b887db6",
   "metadata": {},
   "source": [
    "**Final Goodness Score Calculation**\n",
    "\n",
    "Here, we assign the weights to the goodness formula factors based on our arxiv preprint at https://arxiv.org/abs/2409.06150\n",
    "\n",
    "We have performed two iterations over humans (physicians) to retrieve these weights via Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "38fb577d",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha, beta, lambd, theta = 0.15, 0.22, 0.31, 0.27\n",
    "\n",
    "combo = combination_score(concept_x)\n",
    "freq = get_pubmed_freq(concept_x)\n",
    "norm_freq = normalize_freq(freq, seed_stats)\n",
    "wc = len(concept_x.replace('_', ' ').split())\n",
    "norm_wc = normalize_wc(wc, seed_stats)\n",
    "german = translate_to_german(concept_x)\n",
    "tscore = translation_score(concept_x, german)\n",
    "\n",
    "goodness = (alpha * combo) + (beta * norm_wc) + (lambd * tscore) + (theta * norm_freq)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc90f93",
   "metadata": {},
   "source": [
    "## Stage 2 - Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5a5886c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Stage 2 ===\n",
      "German Translation: schlecht gepflegte städtische öffentliche Parks\n",
      "Raw Word Count: 5 (Seed min: 1.0, max: 6.0)\n",
      "Raw PubMed Frequency: 0 (Seed min: 0.0, max: 533427.0)\n",
      "\n",
      "Combination Score: 0.4\n",
      "Normalized Word Count: 0.2\n",
      "Translation Quality Score: 1.0\n",
      "Normalized Frequency: 0\n",
      "\n",
      "🎯 Final Goodness Score: 0.414 [Weights: α=0.15, β=0.22, λ=0.31, θ=0.27]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Stage 2 ===\")\n",
    "print(f\"German Translation: {german}\")\n",
    "print(f\"Raw Word Count: {wc} (Seed min: {seed_stats['min_wc']}, max: {seed_stats['max_wc']})\")\n",
    "print(f\"Raw PubMed Frequency: {freq} (Seed min: {seed_stats['min_freq']}, max: {seed_stats['max_freq']})\\n\")\n",
    "\n",
    "print(f\"Combination Score: {round(combo, 3)}\")\n",
    "print(f\"Normalized Word Count: {round(norm_wc, 3)}\")\n",
    "print(f\"Translation Quality Score: {round(tscore, 3)}\")\n",
    "print(f\"Normalized Frequency: {round(norm_freq, 3)}\")\n",
    "\n",
    "print(f\"\\n🎯 Final Goodness Score: {round(goodness, 4)} [Weights: α={alpha}, β={beta}, λ={lambd}, θ={theta}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d680c05",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d451ee1",
   "metadata": {},
   "source": [
    "## Concept Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a50c19",
   "metadata": {},
   "source": [
    "Let’s break this down into our 3 categories:\n",
    "\n",
    "⸻\n",
    "\n",
    "**Categories:**\n",
    "\n",
    "- Pass Stage 1 and Stage 2 → Accept as good concept.\n",
    "- Pass Stage 1 only → Useful but may need refinement or more validation.\n",
    "- Fail Stage 1 → Likely irrelevant or redundant concept.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba992e6",
   "metadata": {},
   "source": [
    "**Thresholds**\n",
    "\n",
    "We find these thresholds via two individual experiements mentioned in another folder in our GitHub repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "71338071",
   "metadata": {},
   "outputs": [],
   "source": [
    "STAGE1_THRESHOLD = 0.1710  # Utility Score threshold\n",
    "STAGE2_THRESHOLD = 0.4879   # Goodness Score threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9005bf6e",
   "metadata": {},
   "source": [
    "**Classification Result**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "137a0c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Concept Classification ===\n",
      "⚠️ Passes Stage 1 only – Semantically relevant but lacks other goodness criteria.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Concept Classification ===\")\n",
    "if utility_score >= STAGE1_THRESHOLD and goodness >= STAGE2_THRESHOLD:\n",
    "    print(\"✅ Passes Stage 1 and Stage 2 – Strong candidate for inclusion.\")\n",
    "elif utility_score >= STAGE1_THRESHOLD and goodness < STAGE2_THRESHOLD:\n",
    "    print(\"⚠️ Passes Stage 1 only – Semantically relevant but lacks other goodness criteria.\")\n",
    "else:\n",
    "    print(\"❌ Fails Stage 1 – Not a useful or unique enough concept.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5570fb29",
   "metadata": {},
   "source": [
    "GOOD- climate_change, noise_regulations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170ced89",
   "metadata": {},
   "source": [
    "BAD - housing_pressure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d26114",
   "metadata": {},
   "source": [
    "only stage 1 - badly_maintained_urban_public_parks\n",
    "Extremely verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f6294e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Concept Env",
   "language": "python",
   "name": "concept_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
