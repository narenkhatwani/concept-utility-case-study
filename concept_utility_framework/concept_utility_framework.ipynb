{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8beaa60e",
   "metadata": {},
   "source": [
    "# Concept Utility Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363d6da2",
   "metadata": {},
   "source": [
    "### ENRICH ENDOH CSV WITH WORD COUNT + PUBMED FREQUENCY (LAST 5 YEARS) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fbc21e",
   "metadata": {},
   "source": [
    "**Importing necessary dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698a79a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Bio import Entrez\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dd7c9e",
   "metadata": {},
   "source": [
    "**Load original ENDOH.csv**\n",
    "\n",
    "EnDOH.csv consists of the current version of the Environmental Determinants of Health (EnDOH) from BioPortal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c75689",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"ENDOH.csv\")\n",
    "df = df[['Preferred Label', 'Parents']].dropna()\n",
    "df['Preferred Label'] = df['Preferred Label'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b20186",
   "metadata": {},
   "source": [
    "**Compute Word Count (treat underscores as spaces)**\n",
    "\n",
    "Here we compute the word count for each concept present in EnDOH "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3010998a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Word Count'] = df['Preferred Label'].apply(lambda x: len(x.replace('_', ' ').split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104c8a8c",
   "metadata": {},
   "source": [
    "**Set up Entrez API**\n",
    "\n",
    "API keys to retrieve PubMed frequency of occurence for all the concepts present in the seed ontology (EnDOH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3148bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Entrez.api_key = \"ba9c6cd0806a467f30ca76b5ebd32531b508\"\n",
    "Entrez.email = \"name@example.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e69a37",
   "metadata": {},
   "source": [
    "**Compute PubMed Frequency for the last 5 years**\n",
    "\n",
    "Here we are targeting the retrieval of PubMed Frequency of occurence of a concept for the last 5 year as its necessary to consider the recent developemnts/presence of concepts in the research domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbfed9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year = datetime.now().year - 5\n",
    "end_year = datetime.now().year\n",
    "\n",
    "def get_pubmed_freq(term):\n",
    "    try:\n",
    "        query = f'\"{term.replace(\"_\", \" \")}\" AND ({start_year}[PDAT] : {end_year}[PDAT])'\n",
    "        handle = Entrez.esearch(db=\"pubmed\", term=query, retmax=1)\n",
    "        record = Entrez.read(handle)\n",
    "        handle.close()\n",
    "        return int(record[\"Count\"])\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "# Apply to all concepts\n",
    "frequencies = []\n",
    "for idx, term in enumerate(df['Preferred Label']):\n",
    "    freq = get_pubmed_freq(term)\n",
    "    frequencies.append(freq)\n",
    "    if idx % 10 == 0:\n",
    "        print(f\"Processed {idx + 1} of {len(df)}\")\n",
    "    time.sleep(0.34)  # stay within NCBI limit\n",
    "\n",
    "df['Frequency'] = frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2613b9",
   "metadata": {},
   "source": [
    "**Save enriched file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff8f24a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1 of 102\n",
      "Processed 11 of 102\n",
      "Processed 21 of 102\n",
      "Processed 31 of 102\n",
      "Processed 41 of 102\n",
      "Processed 51 of 102\n",
      "Processed 61 of 102\n",
      "Processed 71 of 102\n",
      "Processed 81 of 102\n",
      "Processed 91 of 102\n",
      "Processed 101 of 102\n",
      "✅ ENDOH_enriched.csv saved with Word Count + Frequency (5-year window).\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(\"ENDOH_enriched.csv\", index=False)\n",
    "print(\"✅ ENDOH_enriched.csv saved with Word Count + Frequency (5-year window).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f03a04f",
   "metadata": {},
   "source": [
    "### CELL 1: Stage 1 - Utility Score (US)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22c8ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07adcae3",
   "metadata": {},
   "source": [
    "**Load enriched CSV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe103ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"ENDOH_enriched.csv\")\n",
    "df = df[['Preferred Label', 'Parents', 'Word Count', 'Frequency']].dropna()\n",
    "df['Preferred Label'] = df['Preferred Label'].astype(str)\n",
    "df['Cluster'] = df['Parents'].apply(lambda x: x.strip().split('#')[-1])\n",
    "cluster_dict = df.groupby('Cluster')['Preferred Label'].apply(list).to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab950f9",
   "metadata": {},
   "source": [
    "**Concept and weights**\n",
    "\n",
    "Here we add weights to the Utility Score formula's two parameters which are \n",
    "\n",
    "\t•\tSemantic Similarity to a cluster (how well this concept fits into an existing topic group) - w1\n",
    "\t•\tRedundancy (how much this concept overlaps in wording with existing concepts, which is bad) - w2\n",
    "    \n",
    "**concept_x** represents the concept we want to evaluate for inclusion in the ontology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73901a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_x = \"badly_maintained_urban_public_parks\"\n",
    "w1 = 1.0  # Semantic similarity\n",
    "w2 = 1.0  # Redundancy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cad3550",
   "metadata": {},
   "source": [
    "**Embedding model**\n",
    "\n",
    "Here we Load a pre-trained sentence embedding model from the Sentence Transformers library. 'all-MiniLM-L6-v2' is a lightweight and fast model with good performance for semantic similarity tasks.\n",
    "\n",
    "- Convert the concept from underscore_case to normal spaced text for better language model interpretation.\n",
    "- Then encode it into a vector (embedding) that represents the meaning of the phrase.\n",
    "- This embedding will later be compared to other concept embeddings to measure similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b09cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "x_embedding = model.encode(concept_x.replace('_', ' '), convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b824b88",
   "metadata": {},
   "source": [
    "**Semantic similarity**\n",
    "\n",
    "For each sub hierarchy, it computes the average cosine similarity between the embedding of concept_x and the embeddings of all concepts already present in that cluster. Before encoding, underscores in concept labels are replaced with spaces to better match the language model’s training data. \n",
    "\n",
    "The Sentence-BERT model (all-MiniLM-L6-v2) is used to generate these embeddings. The resulting average similarity score represents how close in meaning the test concept is to that cluster. \n",
    "\n",
    "The sub hierarchy with the highest average similarity is selected as the “best fit” for concept_x, and this highest similarity value becomes the best_similarity score. This value is a key component of the Stage 1 utility score, as it reflects the semantic alignment of the new concept with existing structures in the ontology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1f28b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_scores = {}\n",
    "for cluster_name, terms in cluster_dict.items():\n",
    "    cluster_embeddings = model.encode([t.replace('_', ' ') for t in terms], convert_to_tensor=True)\n",
    "    cosine_scores = util.cos_sim(x_embedding, cluster_embeddings)\n",
    "    avg_score = torch.mean(cosine_scores).item()\n",
    "    semantic_scores[cluster_name] = avg_score\n",
    "\n",
    "best_cluster = max(semantic_scores, key=semantic_scores.get)\n",
    "best_similarity = semantic_scores[best_cluster]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d25e873",
   "metadata": {},
   "source": [
    "**Redundancy (Jaccard) — only use the best (semantically similar) sub hierarchy**\n",
    "\n",
    "This section calculates the redundancy of the concept based on Jaccard similarity, but only within the cluster identified as the most semantically similar (i.e., the best sub-hierarchy for concept_x). Redundancy is measured by comparing the words in concept_x to the words in each concept already present in that cluster. For each comparison, the Jaccard similarity is computed as the size of the intersection divided by the size of the union of the word sets. The highest redundancy score across all comparisons is retained as max_redundancy. \n",
    "\n",
    "By focusing only on the most semantically relevant cluster, the algorithm avoids penalizing the concept for similarities with unrelated parts of the ontology, and ensures that redundancy is assessed in the most contextually appropriate way. This score is subtracted from the semantic similarity to calculate the final utility score in Stage 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12036929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_sim(a, b):\n",
    "    a_words = set(a.lower().split('_'))\n",
    "    b_words = set(b.lower().split('_'))\n",
    "    union = a_words | b_words\n",
    "    intersection = a_words & b_words\n",
    "    return len(intersection) / len(union) if union else 0\n",
    "\n",
    "max_redundancy = -1\n",
    "most_redundant_concept = \"\"\n",
    "for term in cluster_dict[best_cluster]:\n",
    "    score = jaccard_sim(concept_x, term)\n",
    "    if score > max_redundancy and term != concept_x:\n",
    "        max_redundancy = score\n",
    "        most_redundant_concept = term"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1071b8c",
   "metadata": {},
   "source": [
    "**Utility Score**\n",
    "\n",
    "Calculate the **utility_score** based on the **semantic similarity score** and the **maximum redundancy score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa064e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "utility_score = (w1 * best_similarity) - (w2 * max_redundancy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8e083d",
   "metadata": {},
   "source": [
    "**Word count & frequency stats from seed ontology**\n",
    "\n",
    "This code calculates descriptive statistics for the two key quantitative features used in Stage 2 of the Goodness Score: word count and PubMed frequency. By using the .describe() function on the entire seed ontology, it extracts values such as the mean, standard deviation, minimum, and maximum for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33af13c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_stats = df['Word Count'].describe()\n",
    "freq_stats = df['Frequency'].describe()\n",
    "\n",
    "seed_stats = {\n",
    "    'mean_wc': word_stats['mean'], 'std_wc': word_stats['std'],\n",
    "    'min_wc': word_stats['min'], 'max_wc': word_stats['max'],\n",
    "    'mean_freq': freq_stats['mean'], 'std_freq': freq_stats['std'],\n",
    "    'min_freq': freq_stats['min'], 'max_freq': freq_stats['max']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937f9ea3",
   "metadata": {},
   "source": [
    "## Stage 1 - Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7b624b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Stage 1 ===\n",
      "Concept: badly_maintained_urban_public_parks\n",
      "Most Similar Cluster: Accessibility_to_green_space\n",
      "Avg. Semantic Similarity: 0.5462\n",
      "Max Redundancy (Jaccard): 0.125 with 'loss_of_urban_forest'\n",
      "Utility Score (US): 0.4212\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Stage 1 ===\")\n",
    "print(\"Concept:\", concept_x)\n",
    "print(\"Most Similar Cluster:\", best_cluster)\n",
    "print(\"Avg. Semantic Similarity:\", round(best_similarity, 4))\n",
    "print(\"Max Redundancy (Jaccard):\", round(max_redundancy, 4), f\"with '{most_redundant_concept}'\")\n",
    "print(\"Utility Score (US):\", round(utility_score, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe34c3d",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56307be",
   "metadata": {},
   "source": [
    "### CELL 2: Stage 2 - Goodness Score (Improved) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a63ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from Bio import Entrez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a72449f",
   "metadata": {},
   "outputs": [],
   "source": [
    "API Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d412cf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "merriam_key = \"18cad792-e991-4203-a8a7-b41746f1d538\"\n",
    "entrez_key = \"ba9c6cd0806a467f30ca76b5ebd32531b508\"\n",
    "Entrez.api_key = entrez_key\n",
    "Entrez.email = \"nk88@njit.edu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac34fc60",
   "metadata": {},
   "source": [
    "Valid POS Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ade3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_combos = {\n",
    "    frozenset(['noun', 'noun']): 1.0,\n",
    "    frozenset(['adjective', 'noun']): 0.95,\n",
    "    frozenset(['noun', 'noun', 'noun']): 0.9,\n",
    "    frozenset(['verb', 'noun']): 0.85,\n",
    "    frozenset(['noun', 'adjective']): 0.8,\n",
    "    frozenset(['noun', 'verb']): 0.75,\n",
    "    frozenset(['adjective', 'noun', 'noun']): 0.7,\n",
    "    frozenset(['adjective', 'adjective', 'noun']): 0.65,\n",
    "    frozenset(['noun', 'prepositional phrase']): 0.6,\n",
    "    frozenset(['adjective', 'adjective', 'adjective', 'noun']): 0.55,\n",
    "    frozenset(['noun', 'noun', 'prepositional phrase']): 0.5,\n",
    "    frozenset(['adjective', 'noun', 'noun', 'noun']): 0.45,\n",
    "    frozenset(['noun', 'noun', 'noun', 'noun']): 0.4,\n",
    "    frozenset(['noun', 'adjective', 'noun', 'noun']): 0.35,\n",
    "    frozenset(['adjective', 'noun', 'noun', 'noun', 'noun']): 0.3\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783c4b7a",
   "metadata": {},
   "source": [
    "Merriam-Webster Combination Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adea0355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_merriam(term):\n",
    "    url = f\"https://www.dictionaryapi.com/api/v3/references/medical/json/{term}?key={merriam_key}\"\n",
    "    try:\n",
    "        r = requests.get(url).json()\n",
    "        for entry in r:\n",
    "            if isinstance(entry, dict) and 'meta' in entry and entry['meta']['id'] == term:\n",
    "                if 'fl' in entry:\n",
    "                    return entry['fl']\n",
    "        return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def combination_score(term):\n",
    "    tags = []\n",
    "    for word in term.replace('_', ' ').split():\n",
    "        tag = check_merriam(word)\n",
    "        if tag:\n",
    "            tags.append(tag)\n",
    "    pos_set = frozenset(tags)\n",
    "    for combo in valid_combos:\n",
    "        if pos_set.issubset(combo):\n",
    "            return valid_combos[combo]\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c1f17c",
   "metadata": {},
   "source": [
    "Normalize Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f50cc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_freq(f, stats):\n",
    "    return max(0, min(1, (f - stats['min_freq']) / (stats['max_freq'] - stats['min_freq'])))\n",
    "\n",
    "def normalize_wc(wc, stats):\n",
    "    return max(0, min(1, 1 - ((wc - stats['min_wc']) / (stats['max_wc'] - stats['min_wc']))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e842dbd",
   "metadata": {},
   "source": [
    "Google Translate API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384fb67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_to_german(term):\n",
    "    try:\n",
    "        url = \"https://translate.googleapis.com/translate_a/single\"\n",
    "        params = {\"client\": \"gtx\", \"sl\": \"en\", \"tl\": \"de\", \"dt\": \"t\", \"q\": term.replace('_', ' ')}\n",
    "        res = requests.get(url, params=params).json()\n",
    "        return res[0][0][0]\n",
    "    except:\n",
    "        return term"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c617c14c",
   "metadata": {},
   "source": [
    "Word Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d97c74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(term):\n",
    "    if not isinstance(term, str):\n",
    "        return 0\n",
    "    cleaned_term = re.sub(r'[^A-Za-z0-9\\\\s]', '', term)\n",
    "    return len(cleaned_term.split())\n",
    "\n",
    "def extract_words(term):\n",
    "    if not isinstance(term, str):\n",
    "        return set()\n",
    "    cleaned_term = re.sub(r'[^A-Za-z0-9\\\\s]', '', term)\n",
    "    return set(cleaned_term.split())\n",
    "\n",
    "def decompose_german_term(term):\n",
    "    if not isinstance(term, str):\n",
    "        return set()\n",
    "    segments = re.findall(r'[A-ZÄÖÜa-zäöüß]+', term)\n",
    "    return set(segments)\n",
    "\n",
    "def is_compound_word(eng_term, ger_term):\n",
    "    eng_words = extract_words(eng_term)\n",
    "    ger_words = decompose_german_term(ger_term)\n",
    "    return bool(eng_words & ger_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b738c2ff",
   "metadata": {},
   "source": [
    "Translation Quality Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c49be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translation_score(eng_term, ger_term):\n",
    "    eng_wc = word_count(eng_term)\n",
    "    ger_wc = word_count(ger_term)\n",
    "    score = 0.0\n",
    "\n",
    "    # Case 1: Short concepts (1-3 words)\n",
    "    if 1 <= eng_wc <= 3:\n",
    "        if ger_wc <= eng_wc:\n",
    "            score = 1.0\n",
    "        elif ger_wc <= eng_wc + 1:\n",
    "            score = 0.8\n",
    "        else:\n",
    "            score = 0.5\n",
    "\n",
    "    # Case 2: Medium concepts (4-6 words)\n",
    "    elif 4 <= eng_wc <= 6:\n",
    "        if ger_wc < eng_wc:\n",
    "            score = 1.0\n",
    "        elif ger_wc == eng_wc:\n",
    "            score = 0.8\n",
    "        else:\n",
    "            score = 0.5\n",
    "\n",
    "    # Case 3: Longer concepts (7-20 words)\n",
    "    elif 7 <= eng_wc <= 20:\n",
    "        if ger_wc < eng_wc * 0.8:\n",
    "            score = 0.9\n",
    "        elif ger_wc < eng_wc:\n",
    "            score = 0.7\n",
    "        else:\n",
    "            score = 0.4\n",
    "\n",
    "    # Case 4: Very long concepts (21-80 words)\n",
    "    elif 21 <= eng_wc <= 80:\n",
    "        if ger_wc < eng_wc * 0.8:\n",
    "            score = 0.7\n",
    "        else:\n",
    "            score = 0.4\n",
    "\n",
    "    # Bonus: Compound word check\n",
    "    if is_compound_word(eng_term, ger_term):\n",
    "        score += 0.1\n",
    "\n",
    "    return min(score, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b271755c",
   "metadata": {},
   "source": [
    "Final Goodness Score Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "38fb577d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Stage 2 ===\n",
      "German Translation: schlecht gepflegte städtische öffentliche Parks\n",
      "Raw Word Count: 5 (Seed min: 1.0, max: 6.0)\n",
      "Raw PubMed Frequency: 0 (Seed min: 0.0, max: 532625.0)\n",
      "\n",
      "Combination Score: 0.4\n",
      "Normalized Word Count: 0.2\n",
      "Translation Quality Score: 1.0\n",
      "Normalized Frequency: 0\n",
      "\n",
      "🎯 Final Goodness Score: 0.414 [Weights: α=0.15, β=0.22, λ=0.31, θ=0.27]\n"
     ]
    }
   ],
   "source": [
    "alpha, beta, lambd, theta = 0.15, 0.22, 0.31, 0.27\n",
    "\n",
    "combo = combination_score(concept_x)\n",
    "freq = get_pubmed_freq(concept_x)\n",
    "norm_freq = normalize_freq(freq, seed_stats)\n",
    "wc = len(concept_x.replace('_', ' ').split())\n",
    "norm_wc = normalize_wc(wc, seed_stats)\n",
    "german = translate_to_german(concept_x)\n",
    "tscore = translation_score(concept_x, german)\n",
    "\n",
    "goodness = (alpha * combo) + (beta * norm_wc) + (lambd * tscore) + (theta * norm_freq)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc90f93",
   "metadata": {},
   "source": [
    "## Stage 1 - Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5886c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Stage 2 ===\")\n",
    "print(f\"German Translation: {german}\")\n",
    "print(f\"Raw Word Count: {wc} (Seed min: {seed_stats['min_wc']}, max: {seed_stats['max_wc']})\")\n",
    "print(f\"Raw PubMed Frequency: {freq} (Seed min: {seed_stats['min_freq']}, max: {seed_stats['max_freq']})\\n\")\n",
    "\n",
    "print(f\"Combination Score: {round(combo, 3)}\")\n",
    "print(f\"Normalized Word Count: {round(norm_wc, 3)}\")\n",
    "print(f\"Translation Quality Score: {round(tscore, 3)}\")\n",
    "print(f\"Normalized Frequency: {round(norm_freq, 3)}\")\n",
    "\n",
    "print(f\"\\n🎯 Final Goodness Score: {round(goodness, 4)} [Weights: α={alpha}, β={beta}, λ={lambd}, θ={theta}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d680c05",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d451ee1",
   "metadata": {},
   "source": [
    "## Concept Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a50c19",
   "metadata": {},
   "source": [
    "Let’s break this down into our 3 categories:\n",
    "\n",
    "⸻\n",
    "\n",
    "**Categories:**\n",
    "\n",
    "- Pass Stage 1 and Stage 2 → Accept as good concept.\n",
    "- Pass Stage 1 only → Useful but may need refinement or more validation.\n",
    "- Fail Stage 1 → Likely irrelevant or redundant concept.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba992e6",
   "metadata": {},
   "source": [
    "Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "71338071",
   "metadata": {},
   "outputs": [],
   "source": [
    "STAGE1_THRESHOLD = 0.17  # Utility Score threshold\n",
    "STAGE2_THRESHOLD = 0.45   # Goodness Score threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9005bf6e",
   "metadata": {},
   "source": [
    "Classification Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "137a0c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Concept Classification ===\n",
      "⚠️ Passes Stage 1 only – Semantically relevant but lacks other goodness criteria.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Concept Classification ===\")\n",
    "if utility_score >= STAGE1_THRESHOLD and goodness >= STAGE2_THRESHOLD:\n",
    "    print(\"✅ Passes Stage 1 and Stage 2 – Strong candidate for inclusion.\")\n",
    "elif utility_score >= STAGE1_THRESHOLD and goodness < STAGE2_THRESHOLD:\n",
    "    print(\"⚠️ Passes Stage 1 only – Semantically relevant but lacks other goodness criteria.\")\n",
    "else:\n",
    "    print(\"❌ Fails Stage 1 – Not a useful or unique enough concept.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5570fb29",
   "metadata": {},
   "source": [
    "GOOD- climate_change, noise_regulations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170ced89",
   "metadata": {},
   "source": [
    "BAD - housing_pressure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e024b4a",
   "metadata": {},
   "source": [
    "only stage 1 - badly_maintained_urban_public_parks\n",
    "Extremely verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a0b2f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Concept Env",
   "language": "python",
   "name": "concept_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
